{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate vs. Multivariate Imputation\n",
    "\n",
    "One type of imputation algorithm is univariate, which imputes values in the i-th feature dimension using only non-missing values in that feature dimension (e.g. impute.SimpleImputer). \n",
    "\n",
    "By contrast, multivariate imputation algorithms use the entire set of available feature dimensions to estimate the missing values (e.g. impute.IterativeImputer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Feature Imputation\n",
    "\n",
    "The SimpleImputer class provides basic strategies for imputing missing values. \n",
    "\n",
    "Missing values can be imputed with a provided constant value, or using the statistics (mean, median or most frequent) of each column in which the missing values are located. This class also allows for different missing values encodings.\n",
    "\n",
    "The following snippet demonstrates how to replace missing values, encoded as np.nan, using the mean value of the columns (axis 0) that contain the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.         2.        ]\n",
      " [6.         3.66666667]\n",
      " [7.         6.        ]]\n"
     ]
    }
   ],
   "source": [
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])\n",
    "\n",
    "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "print(imp.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SimpleImputer class also supports **sparse matrices**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 2.]\n",
      " [6. 3.]\n",
      " [7. 6.]]\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse as sp\n",
    "X = sp.csc_matrix([[1, 2], [0, -1], [8, 4]])\n",
    "imp = SimpleImputer(missing_values=-1, strategy='mean')\n",
    "imp.fit(X)\n",
    "\n",
    "X_test = sp.csc_matrix([[-1, 2], [6, -1], [7, 6]])\n",
    "print(imp.transform(X_test).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this format is not meant to be used to implicitly store missing values in the matrix because it would densify it at transform time. Missing values encoded by 0 must be used with dense input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SimpleImputer class also supports **categorical data** represented as string values or pandas categoricals when using the 'most_frequent' or 'constant' strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a' 'x']\n",
      " ['a' 'y']\n",
      " ['a' 'y']\n",
      " ['b' 'y']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame([[\"a\", \"x\"],\n",
    "                    [np.nan, \"y\"],\n",
    "                    [\"a\", np.nan],\n",
    "                    [\"b\", \"y\"]], dtype=\"category\")\n",
    "\n",
    "imp = SimpleImputer(strategy=\"most_frequent\")\n",
    "print(imp.fit_transform(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate Feature Imputation\n",
    "\n",
    "A more sophisticated approach is to use the **IterativeImputer** class, which models each feature with missing values as a function of other features, and uses that estimate for imputation. \n",
    "\n",
    "It does so in an iterated round-robin fashion: \n",
    "\n",
    "> At each step, a feature column is designated as output y and the other feature columns are treated as inputs X. \n",
    "\n",
    "> A regressor is fit on (X, y) for known y. Then, the regressor is used to predict the missing values of y. \n",
    "\n",
    "> This is done for each feature in an iterative fashion, and then is repeated for max_iter imputation rounds. \n",
    "\n",
    "> The results of the final imputation round are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.]\n",
      " [ 6. 12.]\n",
      " [ 3.  6.]]\n"
     ]
    }
   ],
   "source": [
    "imp = IterativeImputer(max_iter=10, random_state=0)\n",
    "imp.fit([[1, 2], [3, 6], [4, 8], [np.nan, 3], [7, np.nan]])\n",
    "\n",
    "X_test = [[np.nan, 2], [6, np.nan], [np.nan, 6]]\n",
    "\n",
    "# the model learns that the second feature is double the first\n",
    "print(np.round(imp.transform(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "## Nearest Neighbors Imputation\n",
    "\n",
    "The KNNImputer class provides imputation for filling in missing values using the k-Nearest Neighbors approach. \n",
    "\n",
    "By default, a euclidean distance metric that supports missing values, nan_euclidean_distances, is used to find the nearest neighbors. Each missing feature is imputed using values from n_neighbors nearest neighbors that have a value for the feature. \n",
    "\n",
    "The feature of the neighbors are averaged uniformly or weighted by distance to each neighbor. \n",
    "\n",
    "If a sample has more than one feature missing, then the neighbors for that sample can be different depending on the particular feature being imputed. When the number of available neighbors is less than n_neighbors and there are no defined distances to the training set, the training set average for that feature is used during imputation. \n",
    "If there is at least one neighbor with a defined distance, the weighted or unweighted average of the remaining neighbors will be used during imputation. If a feature is always missing in training, it is removed during transform. For more information on the methodology, see ref. [OL2001].\n",
    "\n",
    "The following snippet demonstrates how to replace missing values, encoded as np.nan, using the mean feature value of the two nearest neighbors of samples with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 2. , 4. ],\n",
       "       [3. , 4. , 3. ],\n",
       "       [5.5, 6. , 5. ],\n",
       "       [8. , 8. , 7. ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "nan = np.nan\n",
    "X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]\n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n",
    "imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "## Marking Imputed Values\n",
    "\n",
    "The **MissingIndicator** transformer is useful to transform a dataset into corresponding binary matrix indicating the presence of missing values in the dataset. \n",
    "\n",
    "This transformation is useful in conjunction with imputation. \n",
    "\n",
    "When using imputation, preserving the information about which values had been missing can be informative. \n",
    "\n",
    "Note that both the SimpleImputer and IterativeImputer have the boolean parameter add_indicator (False by default) which when set to True provides a convenient way of stacking the output of the MissingIndicator transformer with the output of the imputer.\n",
    "\n",
    "NaN is usually used as the placeholder for missing values. \n",
    "\n",
    "However, it enforces the data type to be float. \n",
    "\n",
    "The parameter missing_values allows to specify other placeholder such as integer. \n",
    "\n",
    "In the following example, we will use -1 as missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True, False],\n",
       "       [False,  True,  True],\n",
       "       [False,  True, False]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import MissingIndicator\n",
    "\n",
    "X = np.array([[-1, -1, 1, 3],\n",
    "               [4, -1, 0, -1],\n",
    "               [8, -1, 1, 0]])\n",
    "\n",
    "indicator = MissingIndicator(missing_values=-1)\n",
    "mask_missing_values_only = indicator.fit_transform(X)\n",
    "mask_missing_values_only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features parameter is used to choose the features for which the mask is constructed. \n",
    "\n",
    "By default, it is 'missing-only' which returns the imputer mask of the features containing missing values at fit time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 3], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicator.features_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features parameter can be set to 'all' to return all features whether or not they contain missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True, False, False],\n",
       "       [False,  True, False,  True],\n",
       "       [False,  True, False, False]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicator = MissingIndicator(missing_values=-1, features=\"all\")\n",
    "mask_all = indicator.fit_transform(X)\n",
    "mask_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicator.features_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------\n",
    "\n",
    "## Pipeline Missing Data Imputers in a Scikit Learn ML Design Flow\n",
    "\n",
    "When using the MissingIndicator in a Pipeline, be sure to use the FeatureUnion or ColumnTransformer to add the indicator features to the regular features. \n",
    "\n",
    "First we obtain the iris dataset, and add some missing values to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import FeatureUnion, make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris(return_X_y=True)\n",
    "mask = np.random.randint(0, 2, size=X.shape).astype(np.bool)\n",
    "X[mask] = np.nan\n",
    "X_train, X_test, y_train, _ = train_test_split(X, y, test_size=100, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a FeatureUnion. All features will be imputed using SimpleImputer, in order to enable classifiers to work with this data. \n",
    "\n",
    "Additionally, it adds the the indicator variables from MissingIndicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 8)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = FeatureUnion(\n",
    "     transformer_list=[\n",
    "         ('features', SimpleImputer(strategy='mean')),\n",
    "         ('indicators', MissingIndicator())])\n",
    "\n",
    "transformer = transformer.fit(X_train, y_train)\n",
    "results = transformer.transform(X_test)\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we cannot use the transformer to make any predictions.\n",
    "\n",
    "We should wrap this in a Pipeline with a classifier (e.g., a DecisionTreeClassifier) to be able to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(transformer, DecisionTreeClassifier())\n",
    "clf = clf.fit(X_train, y_train)\n",
    "results = clf.predict(X_test)\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
